{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef69830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcf451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spandana\\miniconda3\\envs\\cv\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the emotion dataset\n",
    "dataset = load_dataset(\"emotion\")\n",
    "\n",
    "# Convert dataset splits to pandas DataFrame\n",
    "train_data = dataset[\"train\"].to_pandas()\n",
    "validation_data = dataset[\"validation\"].to_pandas() if \"validation\" in dataset else None\n",
    "test_data = dataset[\"test\"].to_pandas() if \"test\" in dataset else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4816205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_subset(data):\n",
    "    subset_data = data.groupby(\"label\").head(50)\n",
    "    subset_data = subset_data.reset_index(drop=True)\n",
    "    return subset_data\n",
    "\n",
    "small_subset_train_data = get_small_subset(train_data)\n",
    "small_subset_validation_data = get_small_subset(validation_data) if validation_data is not None else None\n",
    "small_subset_test_data = get_small_subset(test_data) if test_data is not None else None\n",
    "\n",
    "# Convert the subset DataFrames back to DatasetDict object\n",
    "small_subset_train_dataset = Dataset.from_pandas(small_subset_train_data)\n",
    "small_subset_validation_dataset = Dataset.from_pandas(small_subset_validation_data) if small_subset_validation_data is not None else None\n",
    "small_subset_test_dataset = Dataset.from_pandas(small_subset_test_data) if small_subset_test_data is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053689a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_subset_dataset_dict = DatasetDict({\n",
    "    \"train\": small_subset_train_dataset,\n",
    "    \"validation\": small_subset_validation_dataset,\n",
    "    \"test\": small_subset_test_dataset\n",
    "})\n",
    "\n",
    "# Preprocess function for tokenization\n",
    "def preprocess_data(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the text data.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing text data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized text.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65a1ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecddfca1de74d48b1ee4d9ab7b44931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868ecba75b744b88a60215419549891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30157c98655a4ad9870980468376d561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained model checkpoint\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "encoded_data = small_subset_dataset_dict.map(preprocess_data, batched=True, batch_size=5568)\n",
    "encoded_data = encoded_data.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3268c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_set = encoded_data[\"train\"]\n",
    "validation_set = encoded_data[\"validation\"] \n",
    "test_set = encoded_data[\"test\"] \n",
    "\n",
    "# Get the number of labels from the dataset\n",
    "num_labels = len(set(train_set[\"label\"]))\n",
    "\n",
    "# Define label-to-ID and ID-to-label dictionaries\n",
    "label_to_id = {\"joy\": 0, \"sadness\": 1, \"anger\": 2, \"fear\": 3, \"surprise\": 4, \"disgust\": 5}  \n",
    "id_to_label = {0: \"joy\", 1: \"sadness\", 2: \"anger\", 3: \"fear\", 4: \"surprise\", 5: \"disgust\"}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b7793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model for sequence classification with the appropriate number of labels\n",
    "model = (\n",
    "    AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels, id2label=label_to_id, label2id=id_to_label\n",
    "    )\n",
    "    .to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a4a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metric\n",
    "def compute_evaluation_metrics(predictions):\n",
    "    labels = predictions.label_ids\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Define training arguments\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad2c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fine-tuned-model\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=len(train_set) // batch_size,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=validation_set,\n",
    "    compute_metrics=compute_evaluation_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d57ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 05:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.815600</td>\n",
       "      <td>1.785618</td>\n",
       "      <td>0.203333</td>\n",
       "      <td>0.131250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.779100</td>\n",
       "      <td>1.763431</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.189638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.723900</td>\n",
       "      <td>1.730223</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.223351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.684200</td>\n",
       "      <td>1.701584</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.216023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.622800</td>\n",
       "      <td>1.687967</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.275915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.694712519645691,\n",
       " 'eval_accuracy': 0.31666666666666665,\n",
       " 'eval_f1': 0.2920252900261943,\n",
       " 'eval_runtime': 15.5813,\n",
       " 'eval_samples_per_second': 19.254,\n",
       " 'eval_steps_per_second': 0.642,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=test_set)\n",
    "\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e484b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I feel so happy today! \t Predicted Emotion: sadness\n",
      "Sentence: This news is really sad. \t Predicted Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "# Example sentences\n",
    "sentences = [\"I feel so happy today!\", \"This news is really sad.\"]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensors to the appropriate device\n",
    "input_ids = tokenized_sentences[\"input_ids\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "attention_mask = tokenized_sentences[\"attention_mask\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# Map predicted labels to emotions\n",
    "predicted_emotions = [id_to_label[label.item()] for label in predicted_labels]\n",
    "\n",
    "# Print the predicted emotions for each sentence\n",
    "for sentence, emotion in zip(sentences, predicted_emotions):\n",
    "    print(f\"Sentence: {sentence} \\t Predicted Emotion: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11575e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
